{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76064963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from config import (\n",
    "    TRAIN_FD001_PATH,\n",
    "    ARTIFACTS_DIR,\n",
    "    SCALER_PATH,\n",
    "    ARTIFACTS_INFO_PATH,\n",
    "    SENSORS,\n",
    "    COLS,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c4f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(\n",
    "    data_df: pd.DataFrame,\n",
    "    sensor_cols: Sequence[str],\n",
    "    sequence_length: int,\n",
    ") -> npt.NDArray[np.float32]:\n",
    "    \"\"\"\n",
    "    Convert time series data into sliding windows for sequence models.\n",
    "\n",
    "    Groups by engine unit, sorts by time, then builds windows of length\n",
    "    sequence_length over the selected sensor columns.\n",
    "    \"\"\"\n",
    "    if sequence_length <= 0:\n",
    "        raise ValueError(\"sequence_length must be positive\")\n",
    "\n",
    "    required_cols = {\"unit_number\", \"time_in_cycles\"}\n",
    "    missing = required_cols.difference(data_df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"data_df is missing required columns: {missing}\")\n",
    "\n",
    "    # Ensure chronological order per unit\n",
    "    data_sorted = data_df.sort_values(\n",
    "        [\"unit_number\", \"time_in_cycles\"],\n",
    "        kind=\"mergesort\",\n",
    "    )\n",
    "\n",
    "    sequences: list[npt.NDArray[np.float32]] = []\n",
    "\n",
    "    for _, group in data_sorted.groupby(\"unit_number\", sort=False):\n",
    "        values = group.loc[:, sensor_cols].to_numpy(dtype=np.float32)\n",
    "        num_sequences = values.shape[0] - sequence_length + 1\n",
    "        if num_sequences <= 0:\n",
    "            continue\n",
    "\n",
    "        for start in range(num_sequences):\n",
    "            end = start + sequence_length\n",
    "            sequences.append(values[start:end])\n",
    "\n",
    "    if not sequences:\n",
    "        return np.empty((0, sequence_length, len(sensor_cols)), dtype=np.float32)\n",
    "\n",
    "    return np.stack(sequences, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932878b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_1     0.0\n",
      "sensor_10    0.0\n",
      "sensor_18    0.0\n",
      "sensor_19    0.0\n",
      "dtype: float64\n",
      "++++++++++++++++++++++++++++++\n",
      "Index(['sensor_1', 'sensor_10', 'sensor_18', 'sensor_19'], dtype='object')\n",
      "++++++++++++++++++++++++++++++\n",
      "['sensor_1', 'sensor_10', 'sensor_18', 'sensor_19']\n"
     ]
    }
   ],
   "source": [
    "def fit_and_save_artifacts(\n",
    "    data_path: Path = TRAIN_FD001_PATH,\n",
    "    cols: Sequence[str] = COLS,\n",
    "    artifacts_dir: Path = ARTIFACTS_DIR,\n",
    "    scaler_path: Path = SCALER_PATH,\n",
    "    info_path: Path = ARTIFACTS_INFO_PATH,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fit a MinMaxScaler on training data and persist preprocessing artifacts.\n",
    "\n",
    "    The scaler is fit only on non constant sensor columns.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Starting preprocessing using data at %s\", data_path)\n",
    "\n",
    "    # print(data_path)\n",
    "    # return \n",
    "    if not data_path.is_file():\n",
    "        logger.error(\"Data file not found at %s\", data_path)\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found at {data_path}. \"\n",
    "            \"Expected 'train_FD001.txt' under 'data/CMAPSSData/'.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        data_path,\n",
    "        sep=r\"\\s+\",\n",
    "        header=None,\n",
    "        names=cols,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Identify constant columns among sensors\n",
    "    sensor_df = df.loc[:, SENSORS] # : means all row, SENSORS means only columns in SENSORS list\n",
    "\n",
    "\n",
    "    variance = sensor_df.var() # variance for each sensor column, not just one value\n",
    "    cols_to_drop = variance[variance == 0.0].index.to_list()\n",
    "    cols_to_scale = [col for col in SENSORS if col not in cols_to_drop] # we do this because SENSORS is a python list\n",
    "\n",
    "    logger.info(\"Found %d constant sensor columns to drop\", len(cols_to_drop))\n",
    "    logger.debug(\"Columns to drop: %s\", cols_to_drop)\n",
    "    logger.info(\"Found %d sensor columns to scale\", len(cols_to_scale))\n",
    "\n",
    "    # Fit scaler on non constant sensors (training data only)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df.loc[:, cols_to_scale]) # take all rows, only columns to scale, fit scaler on that\n",
    "\n",
    "    # Save artifacts\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    joblib.dump(scaler, scaler_path) \n",
    "    logger.info(\"Scaler saved to %s\", scaler_path)\n",
    "\n",
    "    artifacts_info = {\n",
    "        \"cols_to_drop\": cols_to_drop,\n",
    "        \"cols_to_scale\": cols_to_scale,\n",
    "    }\n",
    "\n",
    "    info_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with info_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(artifacts_info, f, indent=4)\n",
    "\n",
    "    logger.info(\"Artifacts metadata saved to %s\", info_path)\n",
    "    logger.info(\"Preprocessing complete\")\n",
    "\n",
    "\n",
    "fit_and_save_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a2e79d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "mylist = [1,2,3,4,5,6,7,8,9,10]\n",
    "print([x for x in mylist if x % 2 == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812e92df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 09:51:44,024 INFO __main__ - Starting preprocessing using data at C:\\Users\\bayou\\OneDrive\\Dokumente\\projects\\turbofan-mlops-pipeline\\data\\CMAPSSData\\train_FD001.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unit_number  time_in_cycles  op_setting_1  op_setting_2  op_setting_3  \\\n",
      "0            1               1       -0.0007       -0.0004         100.0   \n",
      "1            1               2        0.0019       -0.0003         100.0   \n",
      "2            1               3       -0.0043        0.0003         100.0   \n",
      "3            1               4        0.0007        0.0000         100.0   \n",
      "4            1               5       -0.0019       -0.0002         100.0   \n",
      "\n",
      "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  \\\n",
      "0    518.67    641.82   1589.70   1400.60     14.62  ...     521.66   \n",
      "1    518.67    642.15   1591.82   1403.14     14.62  ...     522.28   \n",
      "2    518.67    642.35   1587.99   1404.20     14.62  ...     522.42   \n",
      "3    518.67    642.35   1582.79   1401.87     14.62  ...     522.86   \n",
      "4    518.67    642.37   1582.85   1406.22     14.62  ...     522.19   \n",
      "\n",
      "   sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  \\\n",
      "0    2388.02    8138.62     8.4195       0.03        392       2388   \n",
      "1    2388.07    8131.49     8.4318       0.03        392       2388   \n",
      "2    2388.03    8133.23     8.4178       0.03        390       2388   \n",
      "3    2388.08    8133.83     8.3682       0.03        392       2388   \n",
      "4    2388.04    8133.80     8.4294       0.03        393       2388   \n",
      "\n",
      "   sensor_19  sensor_20  sensor_21  \n",
      "0      100.0      39.06    23.4190  \n",
      "1      100.0      39.00    23.4236  \n",
      "2      100.0      38.95    23.3442  \n",
      "3      100.0      38.88    23.3739  \n",
      "4      100.0      38.90    23.4044  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "fit_and_save_artifacts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
